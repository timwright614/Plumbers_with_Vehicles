{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya98JTI6SV2E",
        "outputId": "51b5cad0-8f32-4b37-ca94-0c4173b63e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yDZCmddpeSX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from urllib.parse import urlencode\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "from lxml import etree\n",
        "import re\n",
        "import os\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tldextract\n",
        "!pip install jmd_imagescraper\n",
        "from tldextract import extract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh3hjhYM_4xv",
        "outputId": "ea1648ce-0551-4f3d-9793-d3c9e930754f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.8.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-1.5.1 tldextract-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jmd_imagescraper\n",
            "  Downloading jmd_imagescraper-1.0.2-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (7.1.2)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (1.0.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (7.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (1.3.5)\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (1.0.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jmd_imagescraper) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->jmd_imagescraper) (4.6.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jmd_imagescraper) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jmd_imagescraper) (3.0.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jmd_imagescraper) (5.3.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jmd_imagescraper) (7.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jmd_imagescraper) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jmd_imagescraper) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->jmd_imagescraper) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->jmd_imagescraper) (6.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 22.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (2.0.10)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->jmd_imagescraper) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets->jmd_imagescraper) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->jmd_imagescraper) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->jmd_imagescraper) (1.15.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (5.7.16)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (5.6.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (4.11.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (5.7.0)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (1.8.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.15.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (23.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<=3.0.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->jmd_imagescraper) (2.8.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (5.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (1.5.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (2.16.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (5.10.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->jmd_imagescraper) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->jmd_imagescraper) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->jmd_imagescraper) (2022.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jmd_imagescraper) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jmd_imagescraper) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jmd_imagescraper) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jmd_imagescraper) (2.10)\n",
            "Installing collected packages: jedi, jmd-imagescraper\n",
            "Successfully installed jedi-0.18.1 jmd-imagescraper-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class TimeoutException(Exception): pass\n",
        "\n",
        "@contextmanager\n",
        "\n",
        "def time_limit(seconds):\n",
        "    def signal_handler(signum, frame):\n",
        "        raise TimeoutException(\"Timed out!\")\n",
        "    signal.signal(signal.SIGALRM, signal_handler)\n",
        "    signal.alarm(seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.alarm(0)"
      ],
      "metadata": {
        "id": "BHX3pnbAEgbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_headers():\n",
        "    headers = {\n",
        "        'User-Agent': random.choice(\n",
        "            [\n",
        "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36',\n",
        "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',\n",
        "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
        "                'Mozilla/5.0 (Windows NT 10.0; Win 64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',\n",
        "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36',\n",
        "            ]\n",
        "        )          \n",
        "    }\n",
        "    return headers"
      ],
      "metadata": {
        "id": "FELdVtaPPVDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_google_results(query, num_pages):\n",
        "\n",
        "    logs = {}\n",
        "\n",
        "    domain = 'com'\n",
        "    payload = {\n",
        "        'q': query, \n",
        "        'uule': 'w+CAIQICIfTG9uZG9uLCBFbmdsYW5kLCBVbml0ZWQgS2luZ2RvbQ'\n",
        "    }\n",
        "\n",
        "    links = [] \n",
        "\n",
        "    for count in range(0, num_pages * 10, 10):\n",
        "        payload['start'] = count # One page is equal to 10 google results.\n",
        "\n",
        "        params = urlencode(payload)\n",
        "        url = f'https://www.google.{domain}/search?{params}'\n",
        "\n",
        "        # Scrape.\n",
        "        response = requests.get(url=url, headers=random_headers())\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        unwanted_list = ['google', 'search', '#', 'facebook', 'instagram']\n",
        "\n",
        "        for ref in set(soup.findAll('a')):\n",
        "                href  = ref.get('href')\n",
        "                if type(href) == str:\n",
        "                    good_link = True\n",
        "                    for unwanted in unwanted_list:\n",
        "                        if unwanted in href:\n",
        "                            good_link = False\n",
        "                            dict_append_or_create(href, f'Filtered: On {unwanted}', logs)\n",
        "                    \n",
        "                    if good_link and 'http' in href:\n",
        "                        links.append(href)\n",
        "                    else: \n",
        "                        if good_link:\n",
        "                            dict_append_or_create(href, 'Filtered: http', logs)\n",
        "                            \n",
        "    return links, logs"
      ],
      "metadata": {
        "id": "HaLKIDZGwFjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def url_rank(url):\n",
        "    rank = 0\n",
        "    if 'about' not in url.lower():\n",
        "        rank += 200\n",
        "    rank += (len(url))\n",
        "    return rank"
      ],
      "metadata": {
        "id": "A_myXXL8qdyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_domain(url):\n",
        "    tsd, td, tsu = extract(url)\n",
        "    if len(tsd) > 0:\n",
        "        tsd += '.'\n",
        "    if 'https' in url:\n",
        "        domain_url = f'https://{tsd}{td}.{tsu}'\n",
        "    else:\n",
        "        domain_url = f'http://{tsd}{td}.{tsu}'\n",
        "    return domain_url"
      ],
      "metadata": {
        "id": "U2BXLDrDE0tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_append_or_create(key, value, d):\n",
        "    if key not in d.keys():\n",
        "        d[key] = [value]\n",
        "    else:\n",
        "        d[key].append(value)"
      ],
      "metadata": {
        "id": "K8qeaAqqqkmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_about(url):\n",
        "    \n",
        "    logs = {}\n",
        "\n",
        "    tsd, td, tsu = extract(url)\n",
        "    domain_url = get_domain(url)\n",
        "    \n",
        "    try:\n",
        "        response = requests.get(url=domain_url, headers=random_headers())\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        possible_links = []\n",
        "\n",
        "        for ref in set(soup.findAll('a')):\n",
        "            href  = ref.get('href')\n",
        "            if type(href) == str:\n",
        "                if domain_url in href and 'about' in href.lower():\n",
        "                    possible_links.append(href)\n",
        "                else:\n",
        "                    dict_append_or_create(url, f'Filtered: {href} No about or domain ', logs)\n",
        "\n",
        "        possible_links.sort(key=url_rank)\n",
        "                    \n",
        "        if len(possible_links) > 0:\n",
        "            \n",
        "            dict_append_or_create(url, f'Found: {possible_links[0]}', logs)\n",
        "            return possible_links[0], logs\n",
        "    except: \n",
        "        dict_append_or_create(url, f'Failed: Generic, possibly SSL', logs)\n",
        "    return False, logs"
      ],
      "metadata": {
        "id": "WpmVgo_QkXdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def google_search_about(url):\n",
        "    \n",
        "    logs = {}\n",
        "\n",
        "    tsd, td, tsu = extract(url)\n",
        "    domain_url = get_domain(url)\n",
        "\n",
        "    domain_url_trimmed = domain_url.replace('https://', '')\n",
        "    domain_url_trimmed = domain_url_trimmed.replace('http://', '')\n",
        "    \n",
        "    query = f'about us {domain_url_trimmed}'\n",
        "    dict_append_or_create(url, f'Query: {query}', logs)\n",
        "    \n",
        "    domain = 'com'\n",
        "    payload = {\n",
        "        'q': query, \n",
        "        'uule': 'w+CAIQICIfTG9uZG9uLCBFbmdsYW5kLCBVbml0ZWQgS2luZ2RvbQ'\n",
        "    }\n",
        "\n",
        "    params = urlencode(payload)\n",
        "    search_url = f'https://www.google.{domain}/search?{params}'\n",
        "\n",
        "\n",
        "    response = requests.get(url=search_url, headers=random_headers())\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    unwanted_list = ['google', 'search', '#', 'facebook', 'instagram']\n",
        "\n",
        "    possible_links = []\n",
        "\n",
        "    for ref in set(soup.findAll('a')):\n",
        "        href  = ref.get('href')\n",
        "        if type(href) == str:\n",
        "            good_link = True\n",
        "            for unwanted in unwanted_list:\n",
        "                if unwanted in href:\n",
        "                    good_link = False\n",
        "                    dict_append_or_create(url, f'Filtered: {href} on {unwanted}', logs)\n",
        "\n",
        "            if good_link: \n",
        "                if domain_url in href and 'about' in href.lower():\n",
        "                    possible_links.append(href)\n",
        "                else:\n",
        "                    dict_append_or_create(url, f'Filtered: {href} not in domain/about', logs)\n",
        "    \n",
        "    possible_links.sort(key=url_rank)\n",
        "                \n",
        "    if len(possible_links) > 0:\n",
        "        dict_append_or_create(url, f'Found: {possible_links[0]} on via google', logs)\n",
        "        return possible_links[0], logs\n",
        "    \n",
        "    dict_append_or_create(url, f'None found:', logs)\n",
        "    return False, logs"
      ],
      "metadata": {
        "id": "Auo3C6dhpj4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_urls(url):\n",
        "    \n",
        "    logs = {}\n",
        "    img_urls = []\n",
        "    try:\n",
        "        response = requests.get(url, headers=random_headers())\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        img_tags = soup.find_all('img')\n",
        "\n",
        "        dict_append_or_create(url, f'{len(img_tags)} found', logs)\n",
        "\n",
        "        unwanted_list = ['google', 'icon', 'logo', 'banner', 'facebook']\n",
        "\n",
        "        for img in img_tags:\n",
        "            source_found = True\n",
        "            try:\n",
        "                img_url = img[\"data-srcset\"]\n",
        "            except:\n",
        "                try:\n",
        "                    img_url = img[\"data-src\"]\n",
        "                except:\n",
        "                    try:\n",
        "                        img_url = img[\"src\"]\n",
        "                    except:\n",
        "                        try:\n",
        "                            img_url = img[\"data-lazy-src\"]\n",
        "                        except:\n",
        "                            try: \n",
        "                                img_url = img[\"data-fallback-src\"]\n",
        "                            except:\n",
        "                                try: \n",
        "                                    img_url = re.search(r'\\/\\S+?(png|jpg|gif)', str(img)).group(0)\n",
        "                                except:\n",
        "                                    source_found = False\n",
        "                                    dict_append_or_create(url, f'No soucre found for: {img}', logs)\n",
        "            \n",
        "            if source_found:\n",
        "                if 'http' not in img_url:\n",
        "                    img_url = get_domain(url) + img_url\n",
        "                    dict_append_or_create(url, f'Relative path fixed for {img}', logs)\n",
        "                \n",
        "                good_link = True\n",
        "                for unwanted in unwanted_list:\n",
        "                    if unwanted in img_url:\n",
        "                        good_link = False\n",
        "                        dict_append_or_create(url,f'Filtered {img_url} on {unwanted}', logs)\n",
        "                \n",
        "                if good_link:\n",
        "                    re_search = re.search(r'https?:\\/\\/\\S+?(png|jpg|gif)', img_url)\n",
        "                    if re_search and re_search.group(0) not in img_urls:\n",
        "                        img_urls.append(re_search.group(0))\n",
        "                        dict_append_or_create(url,f'{re_search.group(0)} found for {img}', logs)\n",
        "                else:\n",
        "                    dict_append_or_create(url,f'Filtered {img} on no re match or duplicate', logs)\n",
        "    except: \n",
        "        dict_append_or_create(url, f'Failed: Generic, possibly SSL', logs)\n",
        "    \n",
        "    return img_urls, logs"
      ],
      "metadata": {
        "id": "jVDrf0AuAX1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_from_url_list(img_urls, save_dir, threshold_dim, filename_prfx):\n",
        "\n",
        "    count = 1\n",
        "    filename_dict = {}\n",
        "\n",
        "    logs = {}\n",
        "\n",
        "    for url in img_urls:\n",
        "        \n",
        "        filename = filename_prfx + str(count).zfill(5)\n",
        "        filename_dict[filename] = url\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=random_headers())\n",
        "            response_content = response.content\n",
        "            try:\n",
        "                response_content = str(response_content, 'utf-8')\n",
        "                dict_append_or_create(url,f'Unknown try clause enacted', logs)\n",
        "            except UnicodeDecodeError:\n",
        "                i = Image.open(BytesIO(response_content))\n",
        "                width, height = i.size\n",
        "                \n",
        "                if width > threshold_dim and height > threshold_dim:\n",
        "                    with open(f\"{save_dir}/{filename}.jpg\", \"wb+\") as f:\n",
        "                        f.write(response_content)\n",
        "                    count += 1\n",
        "                    dict_append_or_create(url,f'Write attempted as {filename}', logs)\n",
        "                    if os.path.exists(f'{save_dir}/{filename}.jpg'):\n",
        "                        dict_append_or_create(url,f'Write success as {filename}', logs)\n",
        "                else:\n",
        "                    dict_append_or_create(url,f'Undersize: {url}', logs)\n",
        "        except:\n",
        "            dict_append_or_create(url,f'Failed to get content: {url}', logs)\n",
        "    \n",
        "    return filename_dict, logs"
      ],
      "metadata": {
        "id": "fJhZ9IrtK4Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locations = ['London UK', 'Manchester UK', 'Birmingham UK','New York US', \n",
        "             'Los Angeles US', 'Sydney Australia', 'Toronto CA']\n",
        "\n",
        "search_results = []\n",
        "search_logs = {}\n",
        "for loc in locations:\n",
        "    query = f'plumber {loc}'\n",
        "    num_pages = 3\n",
        "    google_search_output = get_google_results(query, num_pages)\n",
        "    search_results.extend(google_search_output[0])\n",
        "    search_logs.update(google_search_output[1]) \n",
        "\n",
        "search_results = list(set(search_results))"
      ],
      "metadata": {
        "id": "8C2S693wPBJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "about_results = []\n",
        "about_logs = {}\n",
        "timeouts = []\n",
        "for url in search_results:\n",
        "    try:\n",
        "        with time_limit(10):\n",
        "            domain_results = find_about(url)\n",
        "        about_logs.update(domain_results[1])\n",
        "    except TimeoutException as e:\n",
        "        timeouts.append(url)\n",
        "    if domain_results[0]:\n",
        "        about_results.append(domain_results[0])\n",
        "    else:\n",
        "        google_results = google_search_about(url)\n",
        "        about_logs.update(google_results[1])\n",
        "        if google_results[0]:\n",
        "            about_results.append(google_results[0])"
      ],
      "metadata": {
        "id": "UltGq3RT3RaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = list(set(search_results + about_results))\n",
        "\n",
        "images_logs = {}\n",
        "images_urls = {}\n",
        "get_img_timeouts = []\n",
        "for url in all_results:\n",
        "\n",
        "    try:\n",
        "        with time_limit(10):\n",
        "            get_images_reults = get_image_urls(url)\n",
        "    \n",
        "        images_urls[url] = get_images_reults[0]\n",
        "        images_logs.update(get_images_reults[1])\n",
        "    \n",
        "    except TimeoutException as e:\n",
        "        get_img_timeouts.append(url)"
      ],
      "metadata": {
        "id": "1ObaE2Vn4yjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_images = 0\n",
        "for value in images_urls.values():\n",
        "    no_images += len(value)\n",
        "\n",
        "no_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOH7UT7L9VOb",
        "outputId": "bdb67f8d-49f4-4838-ac43-72364d3a5643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3815"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_logs = {}\n",
        "catalogue = {}\n",
        "save_dir = '/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images'\n",
        "threshold_dim = 100\n",
        "count = 1\n",
        "save_img_timeouts = []\n",
        "\n",
        "for page_url in list(images_urls.keys()):\n",
        "    \n",
        "    save_logs_len = len(save_logs)\n",
        "    filename_prfx = str(count).zfill(5) + '_'\n",
        "    try:\n",
        "        with time_limit(20):\n",
        "            catalogue[page_url], save_logs[page_url] = save_from_url_list(\n",
        "                images_urls[page_url], save_dir, threshold_dim, filename_prfx)\n",
        "    \n",
        "    except TimeoutException as e:\n",
        "        get_img_timeouts.append(url)\n",
        "    \n",
        "    if len(save_logs) > save_logs_len:\n",
        "        count +=1"
      ],
      "metadata": {
        "id": "PrzmsTT47fHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page_log in save_logs.keys():\n",
        "    print('\\n\\n', page_log)\n",
        "    for sublog in save_logs[page_log].keys():\n",
        "        print(sublog, save_logs[page_log][sublog])"
      ],
      "metadata": {
        "id": "BIfKNQCvWnM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = save_dir + '/Catalogue/' + 'catalogue.pickle'\n",
        "\n",
        "with open(filename, 'wb') as handle:\n",
        "    pickle.dump(catalogue, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open(filename, 'rb') as handle:\n",
        "    test = pickle.load(handle)\n",
        "\n",
        "print(catalogue == test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knP8L28h8au3",
        "outputId": "95f22735-b707-4cfa-dd6d-c842f02e43bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images/'\n",
        "len(os.listdir(save_dir+'NoVehicle')),len(os.listdir(save_dir+'Vehicle'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTRuekd2Ry9r",
        "outputId": "0560ba9e-5def-475b-c6c3-0589515f7800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2372, 134)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open('/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images/NoVehicle/00419_00013.jpg')\n",
        "im.convert(\"RGBA\").save('/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Test/00419_00013.png', format = 'PNG')"
      ],
      "metadata": {
        "id": "EV5V58EXKMJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = []\n",
        "dir = '/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images/Vehicle/'\n",
        "for i in os.listdir(dir):\n",
        "    im = Image.open(dir + i)\n",
        "    types.append((im.format,im.mode))\n",
        "set(types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-JO9NEGZrPM",
        "outputId": "a80205f5-c338-45dc-d0ef-f6687ca061cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('PNG', 'RGBA')}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images_Additional/plumber_fleet'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCX8gypjaCMo",
        "outputId": "b4cdd9f1-f471-423e-c647-121d5c3b8ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images_Additional/plumber_van'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI9L3_Ws02v4",
        "outputId": "b899c12b-c470-41ee-b31e-4657959742cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images_Additional/plumber_vehicle'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH-OR74E09QH",
        "outputId": "18f6c6c2-1fe8-45e9-9c16-46f199d3305f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_additional = False\n",
        "    if run_additional:\n",
        "        from jmd_imagescraper.core import * \n",
        "\n",
        "        base_dir = '/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images_Additional/'\n",
        "        searches = ['plumber van', 'plumber vehicle', 'plumber fleet']\n",
        "        sub_dirs = [search.replace(' ', '_') for search in searches]\n",
        "\n",
        "        for search,sub_dir in zip(searches, sub_dirs):\n",
        "            duckduckgo_search(base_dir, sub_dir, search, max_results=300)"
      ],
      "metadata": {
        "id": "Fn2BnMDI1Aqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vehicle_dir = '/content/drive/MyDrive/Projects/Plumbers_with_Vehicles/Images/Vehicle/'\n",
        "for sub_dir in sub_dirs:\n",
        "    sub_path = base_dir + sub_dir + '/'\n",
        "    files = os.listdir(sub_path)\n",
        "    count = 1\n",
        "    for file in files:\n",
        "        save_as = str(count).zfill(5)\n",
        "        full_path = sub_path + file\n",
        "        im = Image.open(full_path)\n",
        "        im.convert(\"RGBA\").save(f'{vehicle_dir}{sub_dir}_{save_as}.png', format = 'PNG')\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "epn5BIRR7SR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hzygdo-X_6HJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}